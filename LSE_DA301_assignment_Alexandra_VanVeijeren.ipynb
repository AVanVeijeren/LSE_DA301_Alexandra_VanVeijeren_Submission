{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59711a08",
   "metadata": {},
   "source": [
    "### LSE Data Analytics Online Career Accelerator \n",
    "\n",
    "# DA301:  Advanced Analytics for Organisational Impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03291ae9",
   "metadata": {},
   "source": [
    "## Assignment template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90776e",
   "metadata": {},
   "source": [
    "### Scenario\n",
    "You are a data analyst working for Turtle Games, a game manufacturer and retailer. They manufacture and sell their own products, along with sourcing and selling products manufactured by other companies. Their product range includes books, board games, video games and toys. They have a global customer base and have a business objective of improving overall sales performance by utilising customer trends. In particular, Turtle Games wants to understand: \n",
    "- how customers accumulate loyalty points (Week 1)\n",
    "- how useful are remuneration and spending scores data (Week 2)\n",
    "- can social data (e.g. customer reviews) be used in marketing campaigns (Week 3)\n",
    "- what is the impact on sales per product (Week 4)\n",
    "- the reliability of the data (e.g. normal distribution, Skewness, Kurtosis) (Week 5)\n",
    "- if there is any possible relationship(s) in sales between North America, Europe, and global sales (Week 6)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdfeaee",
   "metadata": {},
   "source": [
    "# Week 1 assignment: Linear regression using Python\n",
    "The marketing department of Turtle Games prefers Python for data analysis. As you are fluent in Python, they asked you to assist with data analysis of social media data. The marketing department wants to better understand how users accumulate loyalty points. Therefore, you need to investigate the possible relationships between the loyalty points, age, remuneration, and spending scores. Note that you will use this data set in future modules as well and it is, therefore, strongly encouraged to first clean the data as per provided guidelines and then save a copy of the clean data for future use.\n",
    "\n",
    "## Instructions\n",
    "1. Load and explore the data.\n",
    "    1. Create a new DataFrame (e.g. reviews).\n",
    "    2. Sense-check the DataFrame.\n",
    "    3. Determine if there are any missing values in the DataFrame.\n",
    "    4. Create a summary of the descriptive statistics.\n",
    "2. Remove redundant columns (`language` and `platform`).\n",
    "3. Change column headings to names that are easier to reference (e.g. `renumeration` and `spending_score`).\n",
    "4. Save a copy of the clean DataFrame as a CSV file. Import the file to sense-check.\n",
    "5. Use linear regression and the `statsmodels` functions to evaluate possible linear relationships between loyalty points and age/renumeration/spending scores to determine whether these can be used to predict the loyalty points.\n",
    "    1. Specify the independent and dependent variables.\n",
    "    2. Create the OLS model.\n",
    "    3. Extract the estimated parameters, standard errors, and predicted values.\n",
    "    4. Generate the regression table based on the X coefficient and constant values.\n",
    "    5. Plot the linear regression and add a regression line.\n",
    "6. Include your insights and observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea2c71",
   "metadata": {},
   "source": [
    "## 1. Load and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cc3186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import neccesary libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm \n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c3d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file(s) as reviews.\n",
    "reviews = pd.read_csv('turtle_reviews.csv')\n",
    "\n",
    "# View the DataFrame.\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f96de1-888a-4698-9ec1-23e768642848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sense check the data and identify column data types\n",
    "reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490758c1",
   "metadata": {},
   "source": [
    "- There are no null values. \n",
    "- The target variable for linear regression is in column 4: loyalty_points.\n",
    "- The review and summary columns are object data types but are not categorical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f36bd0-71be-4494-b873-1b1a2f52f258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that there are no null values. \n",
    "reviews.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad16658-b4eb-4080-8084-ea043453ce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics.\n",
    "reviews.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0a8689",
   "metadata": {},
   "source": [
    "In order to check that there are no erroneous values, the numerical data is visualised. Eg: check to see if there are negative entries or nubers outside of a realistic range that need to be investigated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88322aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise numerical column: renumeration\n",
    "sns.histplot(data=reviews, x='remuneration (k£)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcada57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise numerical column: age\n",
    "sns.histplot(data=reviews, x='age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba832a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise numerical column: renumeration\n",
    "sns.histplot(data=reviews, x='spending_score (1-100)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24052ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise numerical column: renumeration\n",
    "sns.histplot(data=reviews, x='loyalty_points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b444dda",
   "metadata": {},
   "source": [
    "All of the numerical columns seem within acceptable ranges, without any obviously incorrect entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d25344d-3aed-4d27-bb24-2142be9c99ef",
   "metadata": {},
   "source": [
    "## 2. Drop columns\n",
    "\n",
    "I will drop the column 'Platform' because all of the reviews in the data were scraped from the Turtle Games website. I will also drop the 'Language' column because all of the reviews are in English. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511b813a-f04f-4c3a-9a11-ad6d7a423525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns.\n",
    "reviews_clean = reviews.drop(columns=['language', 'platform'])\n",
    "\n",
    "# View column names.\n",
    "reviews_clean.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fafd556-c6fa-439b-aac3-0fe332b1eb45",
   "metadata": {},
   "source": [
    "## 3. Rename columns\n",
    "\n",
    "I will remove special characters from 'remuneration (k£)' and 'spending_score (1-100)' to make the data easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06310ed-ab6b-4f6e-8307-bdd3380853f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column headers.\n",
    "reviews_clean.rename(columns={'remuneration (k£)':'renumeration','spending_score (1-100)': 'spending_score'}, inplace=True)\n",
    "\n",
    "# View column names.\n",
    "reviews_clean.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c386d53-d38c-4b24-8883-7d2257320036",
   "metadata": {},
   "source": [
    "## 4. Save the DataFrame as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc1746-570a-47cc-a8a9-fe8b6756a285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CSV file as output.\n",
    "reviews_clean.to_csv('reviews_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d4f35-c1b3-40ab-ba63-c5fc551f3ef5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import new CSV file with Pandas.\n",
    "reviews_final = pd.read_csv('reviews_clean.csv')\n",
    "\n",
    "# View DataFrame.\n",
    "reviews_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f89a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the column names.\n",
    "reviews_final.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c6d544",
   "metadata": {},
   "source": [
    "The new column 'Unnamed: 0' may be the index values from the first time the dataframe was imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30081c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View 'Unnamed: 0'\n",
    "reviews_final['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09579ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Unnamed: 0\n",
    "reviews_final = reviews_final.drop(columns = 'Unnamed: 0')\n",
    "\n",
    "# View the columns in the dataframe\n",
    "reviews_final.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdd7d5f-2501-4e3e-895c-4a02602e078a",
   "metadata": {},
   "source": [
    "## 5. Linear regression\n",
    "\n",
    "A simple multiple linear regression model is fit to the variables spending scores, age and renumeration to try determine if they are good predictors for the number of loyalty points a customer has. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a7f47e",
   "metadata": {},
   "source": [
    "### 5a) spending vs loyalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75863d52-79df-4200-b044-5542db990fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variable.\n",
    "x = reviews_final['spending_score']\n",
    "\n",
    "# Dependent variable.\n",
    "y = reviews_final['loyalty_points']\n",
    "\n",
    "# Check for data for linearity.\n",
    "plt.ylabel('Loyalty Points')\n",
    "plt.xlabel('Spending Score')\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76e8544",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fb248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OLS to find the line of best fit. \n",
    "\n",
    "# Create the formula to pass through the OLS methods.\n",
    "f = 'y ~ x'\n",
    "reviews_OLS = ols(f, data = reviews_final).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a55354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the estimated parameters.\n",
    "print('Parameters: ', reviews_OLS.params)\n",
    "print()\n",
    "\n",
    "# Extract the standard errors.\n",
    "print('Standard errors: ', reviews_OLS.bse)\n",
    "print()\n",
    "\n",
    "# Extract the predicted values.\n",
    "print('Predicted values: ', reviews_OLS.predict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9798e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the x coefficient and constant\n",
    "c = reviews_OLS.params[0]\n",
    "m = reviews_OLS.params[1]\n",
    "\n",
    "# Print the regression table\n",
    "reviews_OLS.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e21d558",
   "metadata": {},
   "source": [
    "Key results for the model:\n",
    "- R-Squared (Coefficient of determination) = 0.452. \n",
    "    - 45.2% of the variation in Loyalty Points that can be explained by Spending. \n",
    "- F Statistic Probability is very close to zero. This means that there is sufficient evidence to conclude, at a significance level as small as 0.01,  that the *model fits the data better than a model with zero predictor variables.* \n",
    "\n",
    "Key results for the variable:\n",
    "- The intercept has a high p-value for the two-sided t test. Using a significance level of 0.05, the null hypothesis that the actual x-intercept is zero cannot be rejected. The x-intercept is not statistically significant.\n",
    "- The independent variable has a p-value = 0 for the two sided t-test. At a significance level of 0.05, the null hypothesis that the true value of the coefficient is zero can be rejected. There is sufficient evidence to indicate that there is a *strong relationship* between spending score and the number of loyalty points that a customer has. \n",
    "- The x coefficient is 33.0617. This means that for a one unit change in customer spending score, we can expect the number of loyalty points the customer has to increase by a factor of 33.0617. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65482334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sense the intercept and coefficient are set correctly. \n",
    "print(c, \" \", m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dde67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear equation and plot the regression model\n",
    "y_pred = m*x + c\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11acad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original data using a scatterplot\n",
    "plt.scatter(x, y)\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(x, y_pred, color='red')\n",
    "\n",
    "# Set plot details\n",
    "plt.title(\"Linear Regression of Loyalty Points on Spending Score\")\n",
    "plt.ylabel('Loyalty Points')\n",
    "plt.xlabel('Spending Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e7cea6",
   "metadata": {},
   "source": [
    "It can be seen from the above plot that the model fits the data reasonably well at low spending scores, but not for high spending scores. The distance between the predicted values and the actual values increases as spending score increases. \n",
    "This corresponds with the low R-Squared value, which indicated that only roughly 46% of the variability in loyalty points can be explained by customer spending score. Considering that the F-test indicated that the model is statistically significant, I conclude that there is a relationship between customer spending score and loyalty points. Other variables may, however, explain loyalty points better. The explanatory accuracy of the model may also be increased by adding variables to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc0f24f",
   "metadata": {},
   "source": [
    "### 5b) renumeration vs loyalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db590005-b90a-4005-875e-dbec56155229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variable.\n",
    "x = reviews_final['renumeration']\n",
    "\n",
    "# Dependent variable.\n",
    "y = reviews_final['loyalty_points']\n",
    "\n",
    "# Check for data for linearity.\n",
    "plt.ylabel('Loyalty Points')\n",
    "plt.xlabel('Renumeration in 1000s')\n",
    "plt.scatter(x2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6693d533",
   "metadata": {},
   "source": [
    "The scatterplot looks similar to the plot for spending score vs loyalty points. I expect that the regression model will yield similar results as the previous model, due to the data being distributed similarly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e9ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OLS to find the line of best fit. \n",
    "\n",
    "# Create the formula to pass through the OLS methods.\n",
    "f = 'y ~ x'\n",
    "reviews_OLS = ols(f, data = reviews_final).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291fb0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the estimated parameters.\n",
    "print('Parameters: ', reviews_OLS.params)\n",
    "print()\n",
    "\n",
    "# Extract the standard errors.\n",
    "print('Standard errors: ', reviews_OLS.bse)\n",
    "print()\n",
    "\n",
    "# Extract the predicted values.\n",
    "print('Predicted values: ', reviews_OLS.predict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d00e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the x coefficient and constant\n",
    "c = reviews_OLS.params[0]\n",
    "m = reviews_OLS.params[1]\n",
    "\n",
    "# Print the regression table\n",
    "reviews_OLS.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1ec7c8",
   "metadata": {},
   "source": [
    "Key results for the model:\n",
    "- R-Squared (Coefficient of determination) = 0.38. \n",
    "    - 38% of the variation in Loyalty Points that can be explained by Renumeration. \n",
    "- F Statistic Probability is very close to zero. This means that there is sufficient evidence to conclude, at a significance level as small as 0.01,  that the *model fits the data better than a model with zero predictor variables.* There is therefore a relationship between renumeration and loyalty points. \n",
    "\n",
    "Key results for the variable:\n",
    "- The intercept has a high p-value for the two-sided t test. Using a significance level of 0.05, the null hypothesis that the actual x-intercept is zero cannot be rejected. The x-intercept is not statistically significant.\n",
    "- The independent variable has a p-value = 0 for the two sided t-test. At a significance level of 0.05, the null hypothesis that the true value of the coefficient is zero can be rejected. There is sufficient evidence to indicate that there is a *strong relationship* between renumeration and the number of loyalty points that a customer has. \n",
    "- The x coefficient is 34.1878. This means that for a one unit change in renumeration, we can expect the number of loyalty points the customer has to increase by a factor of 34.1878. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2204aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sense the intercept and coefficient are set correctly. \n",
    "print(c, \" \", m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c61c0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear equation and plot the regression model\n",
    "y_pred = m*x + c\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2439e542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original data using a scatterplot\n",
    "plt.scatter(x, y)\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(x, y_pred, color='red')\n",
    "\n",
    "# Set plot details\n",
    "plt.title(\"Linear Regression of Loyalty Points on Renumeration\")\n",
    "plt.ylabel('Loyalty Points')\n",
    "plt.xlabel('Renumeration in 1000s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f4fae",
   "metadata": {},
   "source": [
    "Similar to the model fit using spending score, this model fits the data reasonably well for salaries between zero and close to sixty-thousand pounds. The distance between the predicted values and the actual data increases for salaries including and above 60 000 pounds. \n",
    "This may explain why the key results for the overall model conflict with the key results for the coefficient. The low R-squared value for the model with high significance for the coefficiet of x may be because the model does not predict loyalty points for high-earning customers as well as it preicts loyalty points for low-earning customers. \n",
    "\n",
    "Considering that the F-test indicated that the model is statistically significant, I conclude that there is a relationship between customer spending score and loyalty points. Other variables may, however, explain loyalty points better. The explanatory accuracy of the model may also be increased by adding variables to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e1552d",
   "metadata": {},
   "source": [
    "### 5c) age vs loyalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099274ee-8c86-44dc-a8dc-dfbf91e59728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variable.\n",
    "x = reviews_final['age']\n",
    "\n",
    "# Dependent variable.\n",
    "y = reviews_final['loyalty_points']\n",
    "\n",
    "# Check for data for linearity.\n",
    "plt.ylabel('Loyalty Points')\n",
    "plt.xlabel('Age')\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7c151",
   "metadata": {},
   "source": [
    "This data is distributed very differently to the past two variables. It is also seems to be grouped according to age ranges. The data values within age ranges seems to be similarly distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0084f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OLS to find the line of best fit. \n",
    "\n",
    "# Create the formula to pass through the OLS methods.\n",
    "f = 'y ~ x'\n",
    "reviews_OLS = ols(f, data = reviews_final).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb3a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the estimated parameters.\n",
    "print('Parameters: ', reviews_OLS.params)\n",
    "print()\n",
    "\n",
    "# Extract the standard errors.\n",
    "print('Standard errors: ', reviews_OLS.bse)\n",
    "print()\n",
    "\n",
    "# Extract the predicted values.\n",
    "print('Predicted values: ', reviews_OLS.predict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc52bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the x coefficient and constant\n",
    "c = reviews_OLS.params[0]\n",
    "m = reviews_OLS.params[1]\n",
    "\n",
    "# Print the regression table\n",
    "reviews_OLS.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43fbc9c",
   "metadata": {},
   "source": [
    "Key results for the model:\n",
    "\n",
    "- R-Squared (Coefficient of determination) = 0.002.\n",
    "    -0.2% of the variation in Loyalty Points that can be explained by Age.\n",
    "- F Statistic Probability is 0.0577. At a significance level of 0.05, there is not sufficient evidence to reject the null hypothesis that a zero variable model will fit the data better than this one. There is not a strong explantory relationship between age and loyalty points. \n",
    "\n",
    "Key results for the variable:\n",
    "- The intercept has a p-value of zero for the two-sided t test. The x-intercept is statistcally significant. The F-test for the model suggests that a no-variable models fits the data better than the constructed here. This is why the intercept is statistically significant. \n",
    "- The independent variable has a p-value = 0.058 for the two sided t-test. At a significance level of 0.05, the null hypothesis that the true value of the coefficient is zero cannot be rejected. There is not sufficient evidence to indicate that there is a strong relationship between renumeration and the number of loyalty points that a customer has.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e08b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sense check the intercept and coefficient are set correctly. \n",
    "print(c, \" \", m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6452023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear equation and plot the regression model\n",
    "y_pred = m*x + c\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76161508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original data using a scatterplot\n",
    "plt.scatter(x, y)\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(x, y_pred, color='red')\n",
    "\n",
    "# Set plot details\n",
    "plt.title(\"Linear Regression of Loyalty Points on Age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b2c7d7",
   "metadata": {},
   "source": [
    "Age is not a good predictor of a customer's loyalty points accumulation. The above plot shows that the data is not clustered close to the regression line. This shows what the key results for the model above indicated: that the model is not a good fit for the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771ee1dc",
   "metadata": {},
   "source": [
    "### Multiple linear regression with Spending Score and Renumeration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3643ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Define the dependent variable\n",
    "y = reviews_final['loyalty_points']\n",
    "\n",
    "# Define the dependent variable\n",
    "X = reviews_final[['renumeration', 'spending_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f82d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "mlr = linear_model.LinearRegression()\n",
    "mlr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the R-squared value.\n",
    "print(\"R-squared: \", mlr.score(X,y))  \n",
    "\n",
    "# Print the intercept.\n",
    "print(\"Intercept: \", mlr.intercept_) \n",
    "\n",
    "# Print the coefficients.\n",
    "print(\"Coefficients:\")  \n",
    "\n",
    "# Map a similar index of multiple containers (to be used as a single entity).\n",
    "list(zip(X, mlr.coef_))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbf1461",
   "metadata": {},
   "source": [
    "This multiple linear regression has a high R^2 value. Roughly 83% of variation in loyalty points can be explained by variation in renumeration and spending score. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1fa5b5-ca8f-4ee8-a22e-532142d5cf52",
   "metadata": {},
   "source": [
    "## 6. Observations and insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713d8150-b5e5-4b96-b138-3437158bc020",
   "metadata": {},
   "source": [
    "***Your observations here...***\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2225cd",
   "metadata": {},
   "source": [
    "Questions for next time:\n",
    "- could renumeration and spending score be used in a multiple linear regression to more accurately explain how loyalty points are accumulated? \n",
    "    \n",
    "    \n",
    "Summary: \n",
    "- Age cannot be used to explain loyalty points. \n",
    "- Renumeration and spending score are averagly good at predicting spending score. Both of their model's F-tests indicate that there is a significant relationship between these variables and how customers accumulate loyalty points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e28c75",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c924ec",
   "metadata": {},
   "source": [
    "# Week 2 assignment: Clustering with *k*-means using Python\n",
    "\n",
    "The marketing department also wants to better understand the usefulness of renumeration and spending scores but do not know where to begin. You are tasked to identify groups within the customer base that can be used to target specific market segments. Use *k*-means clustering to identify the optimal number of clusters and then apply and plot the data using the created segments.\n",
    "\n",
    "## Instructions\n",
    "1. Prepare the data for clustering. \n",
    "    1. Import the CSV file you have prepared in Week 1.\n",
    "    2. Create a new DataFrame (e.g. `df2`) containing the `renumeration` and `spending_score` columns.\n",
    "    3. Explore the new DataFrame. \n",
    "2. Plot the renumeration versus spending score.\n",
    "    1. Create a scatterplot.\n",
    "    2. Create a pairplot.\n",
    "3. Use the Silhouette and Elbow methods to determine the optimal number of clusters for *k*-means clustering.\n",
    "    1. Plot both methods and explain how you determine the number of clusters to use.\n",
    "    2. Add titles and legends to the plot.\n",
    "4. Evaluate the usefulness of at least three values for *k* based on insights from the Elbow and Silhoutte methods.\n",
    "    1. Plot the predicted *k*-means.\n",
    "    2. Explain which value might give you the best clustering.\n",
    "5. Fit a final model using your selected value for *k*.\n",
    "    1. Justify your selection and comment on the respective cluster sizes of your final solution.\n",
    "    2. Check the number of observations per predicted class.\n",
    "6. Plot the clusters and interpret the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e7299b",
   "metadata": {},
   "source": [
    "## 1. Load and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1dc705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069e9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sense check the reviews_final dataframe. \n",
    "reviews_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2645d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file(s) as df2.\n",
    "df2 = pd.read_csv('reviews_clean.csv')\n",
    "df2 = df2.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# View DataFrame.\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806bcab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sense check against reviews_final.\n",
    "reviews_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4317dbb3-bab1-4ad3-9c43-ee9af8ffcc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns.\n",
    "clustering_reviews = df2[['renumeration', 'spending_score']]\n",
    "\n",
    "# View DataFrame.\n",
    "print(clustering_reviews.shape)\n",
    "\n",
    "clustering_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f98e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values and value counts.\n",
    "print(clustering_reviews.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e10d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics.\n",
    "clustering_reviews.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402d616f",
   "metadata": {},
   "source": [
    "- Renumeration is the total income per customer per year in pounds.\n",
    "    - Customers have a mean income of 48 079.06. \n",
    "    - Individual values deviate from this by 23 123.984 pounds. \n",
    "- Spending score is the score assigned to a customer based on their spending habits. \n",
    "    - The average spending score of customers is 50. \n",
    "    - This differs per customer by 26 point, on average. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f971229",
   "metadata": {},
   "source": [
    "## 2. Plot\n",
    "\n",
    "- Are there any visible clusters, correlations and outliers?\n",
    "- Are there any existing relationships between spending score and renumeration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a5b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatterplot with Seaborn.\n",
    "sns.scatterplot(x='renumeration', y='spending_score', data=clustering_reviews).set_title('A scatterplot showing the relationship between spending score and renumeration')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbea564b",
   "metadata": {},
   "source": [
    "- No clear linear positive or linear negative relationship. \n",
    "- There are roughly five areas where data values are loosely clustered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600783ce-5a91-4ca8-993f-29499fcc4918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pairplot with Seaborn.\n",
    "sns.color_palette('Paired')\n",
    "sns.pairplot(clustering_reviews, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def77ba4",
   "metadata": {},
   "source": [
    "- Both spending score and renumeration are multi-modal, as shown by the distinct peaks in the kernel density graphs. \n",
    "- This implies that there may be multiple groups within the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d64b28-fc72-4633-af71-2040de573ece",
   "metadata": {},
   "source": [
    "## 3. Elbow and silhoutte methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe03e195",
   "metadata": {},
   "source": [
    "### 1. The Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd0290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the K-Means class\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a list to store the cluster sizes\n",
    "cs = []\n",
    "\n",
    "# Use a loop with a range from 1 - 10 to test cluster sizes\n",
    "for i in range(1,11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n",
    "    kmeans.fit(clustering_reviews)\n",
    "    cs.append(kmeans.inertia_)\n",
    "    \n",
    "# Plot the SSE for each model against the number of clusters\n",
    "plt.plot(range(1,11), cs, marker='o')\n",
    "plt.title(\"The Elbow Method\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"SSE (Total)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f06f62b",
   "metadata": {},
   "source": [
    "- This graph indicates that the optimum number of clusters for this model is k = 5. \n",
    "\n",
    "### 2. The Silhouette Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce995702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import silhouette_score class from sklearn.\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Create an empty list to store the different cluster sizes.\n",
    "sil = []\n",
    "kmax=10\n",
    "\n",
    "for k in range(2, kmax + 1):\n",
    "    kmeans_s = KMeans(n_clusters = k).fit(clustering_reviews)\n",
    "    labels = kmeans_s.labels_\n",
    "    sil.append(silhouette_score(clustering_reviews, labels, metric = 'euclidean'))\n",
    "    \n",
    "# Plot the silhouette scores against number of clusters. \n",
    "plt.plot(range(2, kmax+1), sil, marker='o')\n",
    "\n",
    "plt.title(\"The Silhouette Method\")\n",
    "plt.xlabel(\"The number of clusters\")\n",
    "plt.ylabel(\"Sil\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82acb0ec",
   "metadata": {},
   "source": [
    "The peak is defined as the optimum number of clusters. The peak here is at 5, indicating that 5 is the optimum number of k. \n",
    "\n",
    "In order to evaluate if five is indeed the optimum number of clusters I will test values on either side of the 5: 4,6. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0fd764",
   "metadata": {},
   "source": [
    "## 4. Evaluate k-means model at different values of *k*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6999186",
   "metadata": {},
   "source": [
    "### Evaluate the model for k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e340aa-fac0-4cd1-8da0-ee5502a81504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a k-means object with 4 clusters. \n",
    "kmeans = KMeans(n_clusters=4, max_iter=15000, init='k-means++', random_state=0)\n",
    "kmeans.fit(clustering_reviews)\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "# Add the cluster value to the data frame. \n",
    "clustering_reviews['K-Means Predicted'] = clusters\n",
    "\n",
    "# Plot the predicted values.\n",
    "sns.pairplot(clustering_reviews, hue='K-Means Predicted', diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a745510c",
   "metadata": {},
   "source": [
    "K = 4 clusters\n",
    "\n",
    "- There are three distinct clusters with little overlap. \n",
    "- Cluster zero does look bigger than the other clusters. In-cluster variation may be larger than what is neccesary. \n",
    "- The kernel distribution for cluster zero for both renumeration and spending score are quite wide. This indicates a large in cluster variation value, as the data is spread out over a long axis. \n",
    "- The goal of clustering is for in-cluster variation to be small. Therefore it would be beneficial to have a larger number of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66197d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a k-means object with 5 clusters. \n",
    "kmeans = KMeans(n_clusters=5, max_iter=15000, init='k-means++', random_state=0)\n",
    "kmeans.fit(clustering_reviews)\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "# Add the cluster value to the data frame. \n",
    "clustering_reviews['K-Means Predicted'] = clusters\n",
    "\n",
    "# Plot the predicted values.\n",
    "sns.pairplot(clustering_reviews, hue='K-Means Predicted', diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5828a1",
   "metadata": {},
   "source": [
    "- Adding one additional cluster changed the distributions of spending score and renumeration.\n",
    "- Group zero no longer has such a long right-ward tail, implying that the in-cluster variation is smaller. \n",
    "- The clusters themselves are evenly defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bdf570-062b-488c-9043-1372d3f6c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a k-means object with 6 clusters. \n",
    "kmeans = KMeans(n_clusters=6, max_iter=15000, init='k-means++', random_state=0)\n",
    "kmeans.fit(clustering_reviews)\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "# Add the cluster value to the data frame. \n",
    "clustering_reviews['K-Means Predicted'] = clusters\n",
    "\n",
    "# Plot the predicted values.\n",
    "sns.pairplot(clustering_reviews, hue='K-Means Predicted', diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50115bd2",
   "metadata": {},
   "source": [
    "- Adding an additional cluster did not change the variation of any of the clusters by a large amount. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa5e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a k-means object with 7 clusters. \n",
    "kmeans = KMeans(n_clusters=7, max_iter=15000, init='k-means++', random_state=0)\n",
    "kmeans.fit(clustering_reviews)\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "# Add the cluster value to the data frame. \n",
    "clustering_reviews['K-Means Predicted'] = clusters\n",
    "\n",
    "# Plot the predicted values.\n",
    "sns.pairplot(clustering_reviews, hue='K-Means Predicted', diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acca8f82",
   "metadata": {},
   "source": [
    "- Once again, adding additional cluster did not reduce the dispersion of the data points from their cluster centres. \n",
    "- Looking at the KDEs for renumeration and spending_score, the standard deviation for each cluster looks quite even across all clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d3307d-3831-4a05-ba52-4cfe24262ec6",
   "metadata": {},
   "source": [
    "## 5. Fit final model and justify your choice\n",
    "\n",
    "From the above analysis, it is clear that k = 5 is the optimal choice of clusters. It is the largest number of clusters that significantly reduces the dispersion between the data points and the centroid of their clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883dc3ac-f6cf-47cd-a779-d365f120207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the final model with k =5. \n",
    "kmeans = KMeans(n_clusters=5, max_iter=15000, init='k-means++', random_state=0)\n",
    "kmeans.fit(clustering_reviews)\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "# Add the cluster value to the data frame. \n",
    "clustering_reviews['K-Means Predicted'] = clusters\n",
    "\n",
    "# Plot the predicted values.\n",
    "sns.pairplot(clustering_reviews, hue='K-Means Predicted', diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8802ea-7690-47e6-b23a-0ee483dcde58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the number of observations per predicted class.\n",
    "clustering_reviews['K-Means Predicted'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac510d9",
   "metadata": {},
   "source": [
    "- Cluster zero is has the most amount of data points. \n",
    "- The other four clusters contain close to 300 data points each. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d2cf2",
   "metadata": {},
   "source": [
    "## 6. Plot and interpret the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cd85f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the clusters.\n",
    "sns.set(rc = {'figure.figsize':(12, 8)})\n",
    "\n",
    "sns.scatterplot(x='renumeration' , \n",
    "                y ='spending_score',\n",
    "                data=clustering_reviews , hue='K-Means Predicted', palette='colorblind').set_title(\n",
    "                'Renumeration vs Spending', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de2b0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the dataframe\n",
    "clustering_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e308714e",
   "metadata": {},
   "source": [
    "In order to identify what market segments these clusters identify, I will merge the original data frame with the cluster values and study the other variables that make these groups unique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f41b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join clusters to main data for context. \n",
    "df2['K-Means Predicted'] = clusters\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397e8075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many females and males are in each cluster? \n",
    "gender_clusters = pd.concat([df2, pd.get_dummies(df2['gender'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94db6584",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_clusters.columns\n",
    "gender_clusters.groupby(['K-Means Predicted'])[['Female', 'Male']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363b02e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the average age and income per cluster? \n",
    "gender_clusters.groupby(['K-Means Predicted'])['age', 'renumeration', 'spending_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce56d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the education level of each cluster? \n",
    "education_clusters = pd.concat([df2, pd.get_dummies(df2['education'])], axis=1)\n",
    "\n",
    "education_clusters.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bc47b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "education_clusters.groupby(['K-Means Predicted'])[['Basic', 'PhD', 'diploma','graduate','postgraduate']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8edbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisit the visualisation of the clusters\n",
    "sns.set(rc = {'figure.figsize':(12, 8)})\n",
    "\n",
    "sns.scatterplot(x='renumeration' , \n",
    "                y ='spending_score',\n",
    "                data=clustering_reviews , hue='K-Means Predicted', palette='colorblind').set_title(\n",
    "                'Renumeration vs Spending', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5625ce",
   "metadata": {},
   "source": [
    "## 7. Discuss: Insights and observations\n",
    "\n",
    "***Your observations here...***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6103404d",
   "metadata": {},
   "source": [
    "- Customers can be split into five clusters based on spending score and renumeration. \n",
    "- I then merged the clustering results to get a clearer picture of who belonged to which clusters. \n",
    "\n",
    "*What characterizes customer segments?*  ($ == pounds in this instance)\n",
    "\n",
    "- Cluster 0:\n",
    "    - has the highest proportion of graduates (aprox 45%)\n",
    "    - the average renumeration is $44 418.79 \n",
    "    - the average age is 42\n",
    "    - average spending score is aprox 50. \n",
    "    - largest cluster. \n",
    "    \n",
    "- Cluster 1:\n",
    "    - also make up the greatest proportion of customers in this cluster. \n",
    "    - av. renumeration is $73 240.28\n",
    "    - av. age is 36\n",
    "    - av. spending score is 82. \n",
    "    - these are high spend high value customers\n",
    "\n",
    "- Cluster 2: \n",
    "    - People holding PhDs, graduates and post grads make up similar proportions of the cluster. \n",
    "    - the av. renumeration is $74 831.21.\n",
    "    - the av spending score is 17.42\n",
    "    - low spend customers with high incomes. \n",
    "    \n",
    " - Cluster 3:\n",
    "     - Are majority graduates, with aprox 42 %. \n",
    "     - av. age is 43.\n",
    "     - av. renumeration is $20 424.35. \n",
    "     - av. spending score is 19.76.\n",
    "     \n",
    "- Cluster 4: \n",
    "    - Are majority graduates, with PhD holders following. \n",
    "    - av. age is 31. \n",
    "    - av. renumeration is $20 353.68\n",
    "    - av. spending score is 79.42. \n",
    "    \n",
    " \n",
    "*Questions for next time*\n",
    "- What is the average age of the data as a whole?\n",
    "- Are there outliers in renumeration, spending score and age that could skew the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23335aa9",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b58c90",
   "metadata": {},
   "source": [
    "# Week 3 assignment: NLP using Python\n",
    "Customer reviews were downloaded from the website of Turtle Games. This data will be used to steer the marketing department on how to approach future campaigns. Therefore, the marketing department asked you to identify the 15 most common words used in online product reviews. They also want to have a list of the top 20 positive and negative reviews received from the website. Therefore, you need to apply NLP on the data set.\n",
    "\n",
    "## Instructions\n",
    "1. Load and explore the data. \n",
    "    1. Sense-check the DataFrame.\n",
    "    2. You only need to retain the `review` and `summary` columns.\n",
    "    3. Determine if there are any missing values.\n",
    "2. Prepare the data for NLP\n",
    "    1. Change to lower case and join the elements in each of the columns respectively (`review` and `summary`).\n",
    "    2. Replace punctuation in each of the columns respectively (`review` and `summary`).\n",
    "    3. Drop duplicates in both columns (`review` and `summary`).\n",
    "3. Tokenise and create wordclouds for the respective columns (separately).\n",
    "    1. Create a copy of the DataFrame.\n",
    "    2. Apply tokenisation on both columns.\n",
    "    3. Create and plot a wordcloud image.\n",
    "4. Frequency distribution and polarity.\n",
    "    1. Create frequency distribution.\n",
    "    2. Remove alphanumeric characters and stopwords.\n",
    "    3. Create wordcloud without stopwords.\n",
    "    4. Identify 15 most common words and polarity.\n",
    "5. Review polarity and sentiment.\n",
    "    1. Plot histograms of polarity (use 15 bins) for both columns.\n",
    "    2. Review the sentiment scores for the respective columns.\n",
    "6. Identify and print the top 20 positive and negative reviews and summaries respectively.\n",
    "7. Include your insights and observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40558b5f",
   "metadata": {},
   "source": [
    "## 1. Load and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary packages.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nltk.download ('punkt').\n",
    "# nltk.download ('stopwords').\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Import Counter.\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85947561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data set as df3.\n",
    "df3 = pd.read_csv('reviews_clean.csv')\n",
    "\n",
    "# View DataFrame.\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86c8b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data set.\n",
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1694122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep necessary columns. Drop unnecessary columns.\n",
    "df4 = df3[['review', 'summary']]\n",
    "\n",
    "# View DataFrame.\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00736320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if there are any missing values.\n",
    "df4.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1300ad7c",
   "metadata": {},
   "source": [
    "There are no null values, and 2000 reviews and summaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936bd63b",
   "metadata": {},
   "source": [
    "## 2. Prepare the data for NLP\n",
    "### 2a) Change to lower case and join the elements in each of the columns respectively (review and summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51d4e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review: Change all to lower case and join with a space.\n",
    "df4['review'] = df4['review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "# Preview the result\n",
    "print(df4['review'].shape)\n",
    "df4['review'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615be2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: Change all to lower case and join with a space.\n",
    "df4['summary'] = df4['summary'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "# Preview the result\n",
    "print(df4['summary'].shape)\n",
    "df4['summary'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be5b39d",
   "metadata": {},
   "source": [
    "### 2b) Replace punctuation in each of the columns respectively (review and summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e14ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all the punctuations in review column.\n",
    "df4['review'] = df4['review'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# View output.\n",
    "df4['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e003fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all the puncuations in summary column.\n",
    "df4['summary'] = df4['summary'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# View output.\n",
    "df4['summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb6d0b2",
   "metadata": {},
   "source": [
    "### 2c) Drop duplicates in both columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d130d799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in both columns\n",
    "print(df4.review.duplicated().sum())\n",
    "print(df4.summary.duplicated().sum())\n",
    "print(df4.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c0a6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates based on both columns (complete observations as duplicates)\n",
    "df4_clean = df4.drop_duplicates()\n",
    "df4_clean = df4_clean.reset_index()\n",
    "\n",
    "# View DataFrame.\n",
    "df4_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730162fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_clean['review'][94]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d13e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates again\n",
    "print(df4_clean.review.duplicated().sum())\n",
    "print(df4_clean.summary.duplicated().sum())\n",
    "print(df4_clean.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a092d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d65408",
   "metadata": {},
   "source": [
    "There are duplicate reviews and duplicate summaries remaining, but there are no complete observations as duplicates. \n",
    "1961 values remain. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee064a9-dbc4-4b82-b6e7-17c6e441fa05",
   "metadata": {},
   "source": [
    "## 3. Tokenise and create wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5111dc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DataFrame (copy DataFrame).\n",
    "df5 = df4_clean\n",
    "\n",
    "# View DataFrame.\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ace8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenisation to both columns.\n",
    "df5['review_tokens'] = df5['review'].apply(word_tokenize)\n",
    "df5['summary_tokens'] = df5['summary'].apply(word_tokenize)\n",
    "\n",
    "# View DataFrame.\n",
    "df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review: Create a word cloud.\n",
    "\n",
    "# Define an empty list of tokens\n",
    "review_tokens =[]\n",
    "\n",
    "# Get range\n",
    "length = 1961\n",
    "\n",
    "for i in range(df5.shape[0]):\n",
    "    review_tokens = review_tokens + df5['review_tokens'][i]\n",
    "\n",
    "# View the list\n",
    "review_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63861492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a string with the words\n",
    "review_tokens_string = ' '\n",
    "\n",
    "for value in review_tokens:\n",
    "    # Add each token word to the string\n",
    "    review_tokens_string = review_tokens_string + value + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af9dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a colour palette\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "# Create a WordCloud object\n",
    "word_cloud = WordCloud(width = 1600, height = 900, \n",
    "                       background_color='white',\n",
    "                       colormap='plasma',\n",
    "                       stopwords='none',\n",
    "                       min_font_size=10).generate(review_tokens_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review: Plot the WordCloud image.                     \n",
    "plt.figure(figsize = (16, 9), facecolor = None) \n",
    "plt.imshow(word_cloud) \n",
    "plt.axis('off') \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cbdfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: Create a word cloud.\n",
    "\n",
    "# Define an empty list of tokens\n",
    "summary_tokens =[]\n",
    "\n",
    "# Get range\n",
    "for i in range(df5.shape[0]):\n",
    "    summary_tokens = summary_tokens + df5['summary_tokens'][i]\n",
    "\n",
    "# View the list\n",
    "summary_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977bce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a string with the words\n",
    "summary_tokens_string = ' '\n",
    "\n",
    "for value in summary_tokens:\n",
    "    # Add each token word to the string\n",
    "    summary_tokens_string = summary_tokens_string + value + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b719f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a colour palette\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "# Create a WordCloud object\n",
    "word_cloud2 = WordCloud(width = 1600, height = 900, \n",
    "                       background_color='white',\n",
    "                       colormap='plasma',\n",
    "                       stopwords='none',\n",
    "                       min_font_size=10).generate(summary_tokens_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53776cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: Plot the WordCloud image.\n",
    "plt.figure(figsize = (16, 9), facecolor = None) \n",
    "plt.imshow(word_cloud2) \n",
    "plt.axis('off') \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88414672",
   "metadata": {},
   "source": [
    "In the above WordClouds, words like 'and', 'the'and 'to' are among the most common words. These are stop words that need to be removed so that the most common, useful words are highlighted. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b5abd1",
   "metadata": {},
   "source": [
    "## 4. Frequency distribution and polarity\n",
    "### 4a) Create frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c1b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the frequency distribution.\n",
    "\n",
    "# Reviews\n",
    "fdist_reviews = FreqDist(review_tokens)\n",
    "\n",
    "# Summary\n",
    "fdist_summary = FreqDist(summary_tokens)\n",
    "\n",
    "# Preview the distribution: reviews\n",
    "fdist_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4428be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the distribution: summary\n",
    "fdist_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb8c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the reviews and summary frequency distribution\n",
    "from collections import Counter\n",
    "\n",
    "counts_reviews = pd.DataFrame(Counter(review_tokens).most_common(15), \n",
    "                             columns=['Word', 'Frequency']).set_index('Word')\n",
    "\n",
    "counts_summary = pd.DataFrame(Counter(summary_tokens).most_common(15), \n",
    "                             columns=['Word', 'Frequency']).set_index('Word')\n",
    "\n",
    "# Show the DataFrame: Reviews\n",
    "counts_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d18915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the DataFrame: Summary\n",
    "counts_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e8fbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reviews distribution\n",
    "# Set the plot type.\n",
    "ax = counts_reviews.plot(kind='barh', figsize=(16, 9), fontsize=12,\n",
    "                 colormap ='plasma')\n",
    "\n",
    "# Set the labels.\n",
    "ax.set_xlabel('Count', fontsize=12)\n",
    "ax.set_ylabel('Word', fontsize=12)\n",
    "ax.set_title(\"Top 15 most common words in online review for Turtle Games\",\n",
    "             fontsize=20)\n",
    "\n",
    "# Draw the bar labels.\n",
    "for i in ax.patches:\n",
    "    ax.text(i.get_width()+.41, i.get_y()+.1, str(round((i.get_width()), 2)),\n",
    "            fontsize=12, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f04e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot summary distribution\n",
    "# Set the plot type.\n",
    "ax = counts_summary.plot(kind='barh', figsize=(16, 9), fontsize=12,\n",
    "                 colormap ='viridis')\n",
    "\n",
    "# Set the labels.\n",
    "ax.set_xlabel('Count', fontsize=12)\n",
    "ax.set_ylabel('Word', fontsize=12)\n",
    "ax.set_title(\"Top 15 most common words in online review summaries for Turtle Games\",\n",
    "             fontsize=20)\n",
    "\n",
    "# Draw the bar labels.\n",
    "for i in ax.patches:\n",
    "    ax.text(i.get_width()+.41, i.get_y()+.1, str(round((i.get_width()), 2)),\n",
    "            fontsize=12, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd574d03-a034-454d-b6c5-89aa764c459a",
   "metadata": {},
   "source": [
    "### 4b) Remove alphanumeric characters and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13ce3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are stand alone characters to remove / not real english words\n",
    "tokens1 = [word for word in review_tokens if not word.isalnum()]\n",
    "\n",
    "tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd28216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all the alpanum characters / not real words in reviews\n",
    "review_tokens1 = [word for word in review_tokens if word.isalnum()] \n",
    "\n",
    "review_tokens1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f63e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are stand alone characters to remove / not real english words\n",
    "tokens2 = [word for word in summary_tokens if not word.isalnum()]\n",
    "\n",
    "tokens2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23c756b",
   "metadata": {},
   "source": [
    "There are no alphanumeric characters to remove from the summary tokens. Stopwords can now be removed from both reviews and summaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a757d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the stopwords from reviews\n",
    "\n",
    "# Download the stop word list.\n",
    "nltk.download ('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Create a set of English stop words.\n",
    "english_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d1ad6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a filtered list of tokens without stop words.\n",
    "review_tokens_clean = [x for x in review_tokens1 if x.lower() not in english_stopwords]\n",
    "\n",
    "# Define an empty string variable.\n",
    "reviews_tokens_string2 = ''\n",
    "\n",
    "for value in review_tokens_clean:\n",
    "    # Add each filtered token word to the string.\n",
    "    reviews_tokens_string2 = reviews_tokens_string2 + value + ' '\n",
    "    \n",
    "reviews_tokens_string2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4727927",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Review all stopwords from summaries\n",
    "\n",
    "# Create a filtered list of tokens without stop words.\n",
    "summary_tokens_clean = [x for x in summary_tokens if x.lower() not in english_stopwords]\n",
    "\n",
    "# Define an empty string variable.\n",
    "summary_tokens_string2 = ''\n",
    "\n",
    "for value in summary_tokens_clean:\n",
    "    # Add each filtered token word to the string.\n",
    "    summary_tokens_string2 = summary_tokens_string2 + value + ' '\n",
    "    \n",
    "summary_tokens_string2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e68b09f-853e-4c9c-8ff9-ba0b33b8c8e3",
   "metadata": {},
   "source": [
    "### 4c) Create wordcloud without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa99b607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wordcloud without stop words: Reviews\n",
    "wordcloud_reviews = WordCloud(width = 1600, height = 900, \n",
    "                background_color ='white', \n",
    "                colormap='plasma', \n",
    "                min_font_size = 10).generate(reviews_tokens_string2) \n",
    "\n",
    "# Plot the WordCloud image.                        \n",
    "plt.figure(figsize = (16, 9), facecolor = None) \n",
    "plt.imshow(wordcloud_reviews) \n",
    "plt.axis('off') \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bf8dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wordcloud without stop words: Summary\n",
    "wordcloud_summary = WordCloud(width = 1600, height = 900, \n",
    "                background_color ='white', \n",
    "                colormap='plasma', \n",
    "                min_font_size = 10).generate(summary_tokens_string2) \n",
    "\n",
    "# Plot the WordCloud image.                        \n",
    "plt.figure(figsize = (16, 9), facecolor = None) \n",
    "plt.imshow(wordcloud_summary) \n",
    "plt.axis('off') \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c0b15b",
   "metadata": {},
   "source": [
    "### 4d) Identify 15 most common words and polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2745e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the frequency distribution.\n",
    "\n",
    "# Reviews\n",
    "fdist_reviews = FreqDist(review_tokens_clean)\n",
    "\n",
    "# Summary\n",
    "fdist_summary = FreqDist(summary_tokens_clean)\n",
    "\n",
    "# Preview the distribution: reviews\n",
    "fdist_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b86bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the distribution: summary\n",
    "fdist_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e1978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the reviews and summary frequency distribution\n",
    "from collections import Counter\n",
    "\n",
    "counts_reviews = pd.DataFrame(Counter(review_tokens_clean).most_common(15), \n",
    "                             columns=['Word', 'Frequency']).set_index('Word')\n",
    "\n",
    "counts_summary = pd.DataFrame(Counter(summary_tokens_clean).most_common(15), \n",
    "                             columns=['Word', 'Frequency']).set_index('Word')\n",
    "\n",
    "# Show the DataFrame: Reviews\n",
    "counts_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5841eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the dataframe: summary\n",
    "counts_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9c0046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reviews distribution\n",
    "# Set the plot type.\n",
    "ax = counts_reviews.plot(kind='barh', figsize=(16, 9), fontsize=12,\n",
    "                 colormap ='plasma')\n",
    "\n",
    "# Set the labels.\n",
    "ax.set_xlabel('Count', fontsize=12)\n",
    "ax.set_ylabel('Word', fontsize=12)\n",
    "ax.set_title(\"Top 15 most common words in online review for Turtle Games\",\n",
    "             fontsize=20)\n",
    "\n",
    "# Draw the bar labels.\n",
    "for i in ax.patches:\n",
    "    ax.text(i.get_width()+.41, i.get_y()+.1, str(round((i.get_width()), 2)),\n",
    "            fontsize=12, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fac648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot summary distribution\n",
    "# Set the plot type.\n",
    "ax = counts_summary.plot(kind='barh', figsize=(16, 9), fontsize=12,\n",
    "                 colormap ='viridis')\n",
    "\n",
    "# Set the labels.\n",
    "ax.set_xlabel('Count', fontsize=12)\n",
    "ax.set_ylabel('Word', fontsize=12)\n",
    "ax.set_title(\"Top 15 most common words in online review summaries for Turtle Games\",\n",
    "             fontsize=20)\n",
    "\n",
    "# Draw the bar labels.\n",
    "for i in ax.patches:\n",
    "    ax.text(i.get_width()+.41, i.get_y()+.1, str(round((i.get_width()), 2)),\n",
    "            fontsize=12, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59291784-3505-41e5-9914-e4ec8914524b",
   "metadata": {},
   "source": [
    "## 5. Review polarity and sentiment: Plot histograms of polarity (use 15 bins) and sentiment scores for the respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84288b8f-aab4-4fff-98d0-aaaf5eef28e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided function.\n",
    "def generate_polarity(comment):\n",
    "    '''Extract polarity score (-1 to +1) for each comment'''\n",
    "    return TextBlob(comment).sentiment[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a143b974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Vader lexicon\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88748412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SentimentAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Create a variable to store the SIA\n",
    "darth_vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b5ee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5['review_tokens'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5848edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_value_tokens = df5['review_tokens'].to_list()\n",
    "summary_value_tokens = df5['summary_tokens'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abc3817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and non-alphanumeric characters from reviews\n",
    "filtered_review_tokens = [[y.lower() for y in filtered_tokens if y.lower() not in english_stopwords and y.isalpha()] for filtered_tokens in reviews_value_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c95c1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sense check \n",
    "filtered_review_tokens[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365246bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and non-alphanumeric characters from summaries\n",
    "filtered_summary_tokens = [[y.lower() for y in filtered_tokens if y.lower() not in english_stopwords and y.isalpha()] for filtered_tokens in summary_value_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29db018",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_summary_tokens[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eea3a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine polarity of both columns. \n",
    "polarityscores_reviews_tokens =\\\n",
    "{\" \".join(_) : darth_vader.polarity_scores(\" \".join(_)) for _ in filtered_review_tokens}\n",
    "\n",
    "# View output.\n",
    "polarityscores_reviews_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e81d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame using the dictionary of polarity scores\n",
    "polarity_scores_reviews = pd.DataFrame(polarityscores_reviews_tokens).T\n",
    "\n",
    "polarity_scores_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82535116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine polarity of summaries. \n",
    "polarityscores_summary_tokens =\\\n",
    "{\" \".join(_) : darth_vader.polarity_scores(\" \".join(_)) for _ in filtered_summary_tokens}\n",
    "\n",
    "# View output.\n",
    "polarityscores_summary_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce943e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame using the dictionary of polarity scores\n",
    "polarity_scores_summary = pd.DataFrame(polarityscores_summary_tokens).T\n",
    "\n",
    "polarity_scores_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba685e4",
   "metadata": {},
   "source": [
    "- I have interpreted sentiment as the overall sentiment of the reviews and summaries. This will be measured using compound scores. \n",
    "- To measure polarity, I will look at the positive, negative and neutral polarity of each review and summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcb45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review: Create a histogram plot with bins = 15.\n",
    "# Histogram of polarity\n",
    "\n",
    "\n",
    "# Histogram of sentiment score\n",
    "sns.histplot(data=polarity_scores_reviews['compound'], bins=15).set(title='The Distribution of Reviews Sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9d1928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: Create a histogram plot with bins = 15.\n",
    "# Histogram of polarity\n",
    "\n",
    "\n",
    "# Histogram of sentiment score\n",
    "sns.histplot(data=polarity_scores_summary['compound'], bins=15).set(title='The Distribution of Summary Sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2523b8",
   "metadata": {},
   "source": [
    "## 6. Identify top 20 positive and negative reviews and summaries respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7c797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 positive summaries.\n",
    "summary_pos = polarity_scores_summary.sort_values(['pos'], ascending=False).reset_index()\n",
    "\n",
    "# View output.\n",
    "summary_pos.loc[0:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e172c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 negative summaries.\n",
    "summary_neg = polarity_scores_summary.sort_values(['neg'], ascending=False).reset_index()\n",
    "\n",
    "# View output.\n",
    "summary_neg.loc[0:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348d8b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 positive reviews.\n",
    "reviews_pos = polarity_scores_reviews.sort_values(['pos'], ascending=False).reset_index()\n",
    "\n",
    "# View output.\n",
    "reviews_pos.loc[0:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7c44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 negative reviews.\n",
    "reviews_neg = polarity_scores_reviews.sort_values(['neg'], ascending=False).reset_index()\n",
    "\n",
    "# View output.\n",
    "reviews_neg.loc[0:19]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b2a108-a8af-4164-9b02-40068c17836d",
   "metadata": {},
   "source": [
    "## 7. Discuss: Insights and observations\n",
    "\n",
    "***Your observations here...***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f4c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3ac5e57",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
